# crawler.py
import os, json, re, time, urllib.parse, requests
from bs4 import BeautifulSoup

BASE_DIR = "public/assets"
os.makedirs(BASE_DIR, exist_ok=True)

# Danh s√°ch domain ƒë∆∞·ª£c ph√©p
SITES = ["dangcongsan.vn", "baochinhphu.vn", "vtv.vn"]

def in_whitelist(url):
    """Ki·ªÉm tra URL c√≥ thu·ªôc 3 ngu·ªìn ƒë∆∞·ª£c ph√©p kh√¥ng"""
    host = urllib.parse.urlparse(url).netloc
    return any(host.endswith(s) for s in SITES)

def fetch_images_from_page(page_url):
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ ·∫£nh t·ª´ m·ªôt trang web"""
    try:
        r = requests.get(page_url, timeout=30)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        imgs = set()
        
        # L·∫•y og:image (·∫£nh ƒë·∫°i di·ªán)
        for tag in soup.select('meta[property="og:image"]'):
            if tag.get("content"):
                imgs.add(urllib.parse.urljoin(page_url, tag["content"]))
        
        # L·∫•y t·∫•t c·∫£ th·∫ª img
        for img_tag in soup.find_all("img"):
            src = img_tag.get("data-src") or img_tag.get("src")
            if src:
                imgs.add(urllib.parse.urljoin(page_url, src))
        
        return list(imgs)
    except Exception as e:
        print(f"L·ªói khi t·∫£i trang {page_url}: {e}")
        return []

def sanitize_filename(name):
    """L√†m s·∫°ch t√™n file"""
    name = name.strip().split("?")[0]
    name = os.path.basename(name)
    return re.sub(r'[^a-zA-Z0-9._-]+','_', name) or f"img_{int(time.time()*1000)}.jpg"

def download_image(url, save_dir=BASE_DIR):
    """T·∫£i ·∫£nh v·ªÅ m√°y"""
    try:
        filename = sanitize_filename(url)
        if not filename.lower().endswith((".jpg",".jpeg",".png",".webp")):
            filename += ".jpg"
        
        filepath = os.path.join(save_dir, filename)
        
        with requests.get(url, stream=True, timeout=60) as r:
            r.raise_for_status()
            with open(filepath, "wb") as f:
                for chunk in r.iter_content(8192):
                    if chunk:
                        f.write(chunk)
        
        return filepath
    except Exception as e:
        print(f"L·ªói khi t·∫£i ·∫£nh {url}: {e}")
        return None

def run():
    """H√†m ch√≠nh th·ª±c hi·ªán crawler"""
    links_path = "sources/links.txt"
    if not os.path.exists(links_path):
        print("‚ùå Thi·∫øu file sources/links.txt")
        return
    
    with open(links_path, "r", encoding="utf-8") as f:
        pages = [line.strip() for line in f if line.strip()]

    downloaded_images = []
    
    for page_url in pages:
        if not in_whitelist(page_url):
            print(f"‚ö†Ô∏è B·ªè qua URL kh√¥ng h·ª£p l·ªá: {page_url}")
            continue
        
        print(f"üîç ƒêang x·ª≠ l√Ω: {page_url}")
        images = fetch_images_from_page(page_url)
        
        for img_url in images:
            try:
                filepath = download_image(img_url)
                if filepath:
                    downloaded_images.append({
                        "local_path": filepath.replace("\\","/"),
                        "source_page": page_url,
                        "image_url": img_url,
                        "credit": page_url,
                        "accessed_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "file_size": os.path.getsize(filepath)
                    })
                    print(f"‚úÖ ƒê√£ l∆∞u: {filepath}")
            except Exception as e:
                print(f"‚ùå L·ªói t·∫£i ·∫£nh {img_url}: {e}")

    # L∆∞u metadata
    with open(os.path.join(BASE_DIR, "images.json"), "w", encoding="utf-8") as f:
        json.dump(downloaded_images, f, ensure_ascii=False, indent=2)
    
    print(f"üéâ Ho√†n th√†nh! ƒê√£ t·∫£i {len(downloaded_images)} ·∫£nh ‚Üí public/assets/images.json")

if __name__ == "__main__":
    run()
